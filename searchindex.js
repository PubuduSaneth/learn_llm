Search.setIndex({"alltitles":{"Anatomy of a LLM":[[0,"anatomy-of-a-llm"]],"Attention weights calculation: softmax(\\frac{QK^T}{\\sqrt{d_k}})":[[5,"attention-weights-calculation-softmax-frac-qk-t-sqrt-d-k"]],"Autoregressive text generation":[[6,"autoregressive-text-generation"]],"Basic (simplest) overview":[[8,"instructor-note-0"]],"Book \u201cBuild a Large Language Model (From Scratch)\u201d, Sebastian Raschka":[[13,"book-build-a-large-language-model-from-scratch-sebastian-raschka"]],"Building Blocks of GPT-2 LLM":[[12,null]],"Coding":[[5,"instructor-note-0"],[7,"instructor-note-0"],[9,"instructor-note-0"]],"Context Window":[[11,"context-window"]],"Context vector":[[5,"context-vector"]],"Credits":[[12,"credits"]],"Data parallelization in LLM":[[11,"data-parallelization-in-llm"]],"Dataflow across LLM":[[11,null]],"Dataflow across the LLM":[[11,"dataflow-across-the-llm"]],"Demo":[[3,"demo-0"],[3,"demo-1"]],"Embedding layer of LLMs":[[3,"embedding-layer-of-llms"]],"Encoder and Decoder architecture":[[4,"encoder-and-decoder-architecture"]],"Explore LLM token embeddings":[[3,"explore-llm-token-embeddings"]],"Extract hidden state of each layer in the model":[[10,"extract-hidden-state-of-each-layer-in-the-model"]],"FNN in the transformer block":[[8,"fnn-in-the-transformer-block"]],"Feedforward neural network (FNN)":[[8,null]],"GPT - Generative Pretrained Transformer model":[[1,null]],"GPT-2":[[1,"gpt-2"]],"GPT-2 Token embeddings: Step-by-Step Breakdown":[[3,"gpt-2-token-embeddings-step-by-step-breakdown"]],"GPT-2 model variants and their components":[[1,"gpt-2-model-variants-and-their-components"]],"GPT-2 tokenizer":[[2,"gpt-2-tokenizer"]],"Hands-On Large Language Models, Jay Alammar, Maarten Grootendorst":[[13,"hands-on-large-language-models-jay-alammar-maarten-grootendorst"]],"How is the LM head help predict next token":[[9,"how-is-the-lm-head-help-predict-next-token"]],"How to apply attention mask?":[[6,"how-to-apply-attention-mask"]],"Introduction":[[12,"introduction"]],"Introduction to Large Language Models (LLMs)":[[0,null]],"Introduction to embedding":[[3,null]],"Introduction to tokenization":[[2,null]],"LLMs":[[0,"llms"]],"LLMs vs other ML approaches":[[0,"llms-vs-other-ml-approaches"]],"Language Modeling Head (LM Head)":[[9,null]],"Language modeling (LM)":[[0,"language-modeling-lm"]],"Limitations and solutions":[[4,"limitations-and-solutions"]],"Limitations in traditional LM":[[4,"limitations-in-traditional-lm"]],"Load Pre-trained GTP-2 model":[[10,"load-pre-trained-gtp-2-model"]],"Main Components of a transformer block":[[4,"main-components-of-a-transformer-block"]],"Main Stages: softmax(\\frac{QK^T}{\\sqrt{d_k}})*V":[[5,"main-stages-softmax-frac-qk-t-sqrt-d-k-v"]],"Main features":[[8,"main-features"]],"Masked Attention":[[6,null]],"Masking during the attention mechanism":[[6,"masking-during-the-attention-mechanism"]],"Multi-head attention mechanism":[[7,"multi-head-attention-mechanism"]],"Multi-head self-attention":[[7,null]],"Next token prediction (via LM-head defined in the model)":[[10,"next-token-prediction-via-lm-head-defined-in-the-model"]],"Objective":[[10,"objective"],[12,"objective"]],"Objectives":[[0,"objectives-0"],[1,"objectives-0"],[2,"objectives-0"],[3,"objectives-0"],[4,"objectives-0"],[5,"objectives-0"],[6,"objectives-0"],[7,"objectives-0"],[8,"objectives-0"],[9,"objectives-0"],[11,"objectives-0"]],"Output":[[2,"instructor-note-0"],[3,"instructor-note-0"],[3,"instructor-note-1"]],"Overview of self-attention with Q, K, V weight matrix":[[5,"overview-of-self-attention-with-q-k-v-weight-matrix"]],"Position embeddings":[[3,"position-embeddings"]],"Pre-trained GPT-2 model end to end":[[10,null]],"Prerequisites":[[12,"prerequisites-0"]],"Prevents \u201cCheating\u201d During Training":[[6,"prevents-cheating-during-training"]],"Queries (Q), Keys (K), and Values (V)":[[5,"queries-q-keys-k-and-values-v"]],"Reference":[[12,null],[13,null]],"Run custom LM-head and extract top 5 predictions":[[10,"run-custom-lm-head-and-extract-top-5-predictions"]],"Run experiment":[[10,"run-experiment"]],"Self-attention mechanism":[[5,null]],"Set custom LM-head":[[10,"set-custom-lm-head"]],"Text to Token-IDs":[[2,"text-to-token-ids"]],"Text to subword units":[[2,"text-to-subword-units"]],"The lesson":[[12,null]],"Token embedding":[[3,"token-embedding"]],"Token embeddings in LLMs":[[3,"token-embeddings-in-llms"]],"Tokenization step-by-step":[[2,"tokenization-step-by-step"]],"Tokenizers":[[2,"tokenizers"]],"Tokenizers: Text to sub-word units (tokens)":[[2,"tokenizers-text-to-sub-word-units-tokens"]],"Transformer blocks":[[4,null]],"Transformer model":[[4,"transformer-model"]],"Transformer technology":[[4,"transformer-technology"]],"Understanding attention equation (scaled Dot-Product Attention)":[[5,"understanding-attention-equation-scaled-dot-product-attention"]],"Understanding self-attention with Q, K, V weight matrix":[[5,"understanding-self-attention-with-q-k-v-weight-matrix"]],"What is LM head?":[[9,"what-is-lm-head"]],"What is Masked Self-Attention?":[[6,"what-is-masked-self-attention"]],"What is Multi-head attention":[[7,"what-is-multi-head-attention"]],"What is a Transformer block?":[[4,"what-is-a-transformer-block"]],"What is self-attention mechanism?":[[5,"what-is-self-attention-mechanism"]],"When is the mask applied?":[[6,"when-is-the-mask-applied"]],"Who is the tutorial for?":[[12,"who-is-the-tutorial-for"]],"Why FNN is important?":[[8,"why-fnn-is-important"]],"Why Is Transformer technology important?":[[4,"why-is-transformer-technology-important"]],"Why LM head is important?":[[9,"why-lm-head-is-important"]],"Why does the LM field needs LLMs?":[[0,"why-does-the-lm-field-needs-llms"]],"Why is it Important?":[[6,"why-is-it-important"]],"Why is it important?":[[7,"why-is-it-important"]],"Wide adaption of decoder transformers":[[4,"wide-adaption-of-decoder-transformers"]]},"docnames":["01.LLM_intro","02.GPT_intro","03.tokenizer","04.Embeddings","05.Transformer_block","06.Attention_mechanism","07.masked_attention","08.Multihead-attention","09.FNN","10.LM_head","10_1.LLM-end-to-end","11.LLM_dataflow","index","quick-reference"],"envversion":{"sphinx":65,"sphinx.domains.c":3,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":9,"sphinx.domains.index":1,"sphinx.domains.javascript":3,"sphinx.domains.math":2,"sphinx.domains.python":4,"sphinx.domains.rst":2,"sphinx.domains.std":2},"filenames":["01.LLM_intro.md","02.GPT_intro.md","03.tokenizer.md","04.Embeddings.md","05.Transformer_block.md","06.Attention_mechanism.md","07.masked_attention.md","08.Multihead-attention.md","09.FNN.md","10.LM_head.md","10_1.LLM-end-to-end.ipynb","11.LLM_dataflow.md","index.md","quick-reference.md"],"indexentries":{},"objects":{},"objnames":{},"objtypes":{},"terms":{"":[0,2,3,4,6,8,9,10,12],"0":[3,9,10],"0005":3,"0073":3,"0090":3,"0098":3,"0141":3,"0427":3,"0456":3,"0464":3,"05":10,"0594":3,"0701":3,"072":9,"0743":3,"0894":3,"0941":3,"0957":3,"0958":3,"1":[1,3,5,6,9,10],"10":3,"1024":[1,10],"1052":3,"11":10,"111":3,"1112":3,"1115":3,"1123":3,"12":[1,10],"1280":1,"1474":3,"15":6,"1542":1,"16":1,"1600":1,"1634":2,"16345":2,"17":1,"17796":3,"1980":0,"1997":0,"1e":10,"1st":3,"2":[5,9],"20":1,"2014":0,"2015":0,"2017":1,"2304":10,"24":1,"25":1,"25192":3,"2527":3,"257":[1,9],"262":[2,3],"3":[1,3,5,9,10],"300":3,"3072":10,"318":3,"319":[2,3],"3331":3,"3332":[2,3],"36":1,"3876":2,"3d":3,"4":[1,3,8,10],"48":1,"4f":3,"5":3,"50":[1,9,10],"50256":3,"50257":[2,3,10],"5431":3,"5472":3,"58":3,"5d":3,"6":3,"62":3,"7":[9,10],"768":[1,3,10],"7850":3,"786":1,"8":3,"84":3,"89":3,"9246":[2,3],"A":[2,5],"As":[3,11],"At":12,"For":3,"In":[2,3,6,7,10],"No":10,"Not":6,"One":7,"The":[1,2,3,4,5,7,11],"These":[0,2,3,5,9],"To":[5,6],"With":[0,6],"_":3,"abil":[0,6],"accept":[2,8],"access":3,"account":8,"achiev":7,"across":[4,12],"act":[8,10],"activ":8,"actual":[5,8],"ad":[2,3],"add":11,"add_subplot":3,"address":[0,4],"advanc":0,"aggreg":5,"align":5,"all":[0,1,3,4,5,7,8,10,11],"all_hidden_st":10,"all_token_hidden_st":10,"allow":[4,5,7],"alon":3,"along":[9,10],"alwai":8,"ambigu":4,"amount":0,"an":[7,9],"ani":6,"anim":3,"anoth":[4,7],"apart":5,"api":3,"appear":[3,6],"append":11,"appli":[3,5,10],"appropri":4,"ar":[0,1,2,3,4,5,6,7,8,9],"arbitrari":3,"architectur":[0,1,12],"aren":3,"argmax":10,"art":4,"aspect":7,"assign":[2,3],"associ":8,"attend":[5,6],"attent":[0,1,3,4,8,12],"attn":10,"attn_dropout":10,"attribut":3,"auto":1,"automodelforcausallm":[3,10],"autoregress":[4,11],"autotoken":[2,10],"avoid":5,"awar":5,"ax":3,"axes3d":3,"back":[3,4,8,9,10,11],"backpropag":3,"balanc":2,"bank":3,"bank_1_id":3,"bank_2_id":3,"base":[0,1,4,6,11],"basic":[4,5,12],"bed":10,"befor":3,"behavior":[0,6],"behaviour":4,"being":0,"bench":10,"bert":[4,6],"between":[3,5,6,8],"bia":[8,10],"billion":0,"bite":3,"blank":6,"block":[0,1,3,6,7,9,11],"both":[4,6],"boundari":2,"break":3,"broader":6,"build":[0,1,2],"bulk":8,"c_attn":10,"c_fc":10,"c_proj":10,"calcul":[3,8,9,10,11],"call":[7,9,10],"can":[0,1,2,3,4,5,6,8,11],"capabl":0,"captur":[3,4,5,7,8,10],"case":2,"cat":[2,3,10],"causal":6,"cell":10,"challeng":0,"chapter":5,"characterist":4,"cluster":3,"code":4,"coher":[4,6],"collect":10,"combin":[3,5,7],"compar":5,"comparison":5,"complex":[0,3,5,8],"compon":[9,12],"compress":8,"comput":[0,2,4,8],"concaten":7,"concept":[4,5,12],"confid":[9,10],"connect":[4,9],"consid":5,"consist":2,"constraint":[6,11],"construct":4,"contain":[5,8],"context":[3,4,6,7,8,9],"contextu":[3,4],"continu":[3,4,6],"contribut":11,"conv1d":10,"convert":[2,3,5,9,10,11],"core":0,"corpu":1,"correspond":2,"cosine_sim":3,"cosine_similar":3,"couldn":3,"cpu":10,"creat":[5,6,7,9,10],"creativ":[9,10],"current":5,"custom":9,"d_k":6,"d_model":9,"d_vocabulari":9,"data":[0,1,2,3,4,8,12],"dataflow":12,"decod":[0,2,3,6,9,10,11,12],"decoded_sent":2,"decomposit":3,"deep":[2,12],"def":[3,9,10],"default":[1,10],"dens":3,"depend":[0,4,5],"describ":6,"design":[4,12],"despit":0,"detail":3,"determin":[4,5,8],"determinist":10,"develop":4,"dictionari":2,"differ":[0,3,5,7],"differenti":[3,4],"dim":[3,9,10],"dimens":[1,3,5,7,8,9,10],"dimension":[3,8],"disabl":10,"discard":4,"discov":5,"discret":[2,3,9,10],"discuss":4,"distinct":[5,7],"distinguish":3,"distribut":[9,10],"dive":12,"divers":2,"divid":7,"do":[0,3],"document":2,"dog":3,"down":8,"download":3,"drop":10,"dropout":10,"dual":4,"due":5,"dure":[3,5,8],"dynam":5,"e":[0,2,3,5,6,7,8,9,12],"each":[2,3,4,5,7,8,9,12],"earli":0,"earliest":11,"edg":10,"effect":[2,3,4],"effici":[2,7],"element":3,"elementwise_affin":10,"elimin":0,"els":10,"embed":[0,1,5,6,9,10,11,12],"embedding_dim":3,"emerg":0,"emploi":[0,9],"enabl":[2,3,4,5,6,9,12],"encod":[0,2,3,6],"end":[4,9,12],"engin":8,"enrich":[5,8,11],"ensur":[5,6,10],"entir":[3,4],"enumer":3,"ep":10,"equal":[2,3,9],"error":3,"essenti":6,"eval":10,"evalu":[3,10],"even":8,"everi":9,"evolv":3,"exampl":3,"exce":11,"execut":7,"exercis":9,"exhibit":0,"expand":8,"explain":[3,5],"explicitli":0,"explor":[0,1,2,4,5,7,8,9,10,11,12],"expos":6,"express":2,"extend":[3,7],"extent":5,"extract":[3,8,9],"f":[2,3,9,10],"fail":4,"fals":10,"far":5,"feed":[4,6,8,11],"feedforward":[0,12],"fig":3,"figsiz":3,"figur":[3,8],"file":0,"fill":[6,9,10],"filler":6,"filter":[9,10],"filtered_logit":[9,10],"final":[3,4,5,7,9,10],"final_token":[9,10],"final_token_id":[9,10],"first":[3,5],"fit_transform":3,"fix":2,"flexibl":8,"float":[9,10],"floor":10,"flow":[3,11],"fnn":12,"focu":[4,5,7,12],"focuss":4,"follow":[3,4],"fontsiz":3,"forc":[6,10],"forget":11,"form":7,"forward":[4,8,11],"foundat":2,"four":1,"frac":6,"from":[0,2,3,4,5,7,8,9,10,12],"from_pretrain":[2,3,10],"full":4,"full_lik":[9,10],"fulli":0,"function":[3,5,6,9,10],"fundament":[4,12],"futur":[6,11],"g":[0,3,6,7,9],"gain":[4,5,12],"gate":0,"gener":[2,3,4,5,7,8,11,12],"gensim":3,"geometr":3,"get":[3,5,9,10],"get_3d_project":3,"get_all_hidden_st":10,"get_output_embed":[9,10],"get_top_token":[9,10],"given":3,"googl":3,"gpt":[4,6,9],"gpt2":[2,3,10],"gpt2attent":10,"gpt2block":10,"gpt2lmheadmodel":10,"gpt2mlp":10,"gpt2model":10,"gradient":[0,3,5],"grammat":7,"ground":10,"guess":6,"guid":4,"h":10,"ha":3,"handl":[0,2,11],"happen":3,"have":[3,5,8,12],"head":[0,1,11,12],"help":[3,5,7],"hidden":8,"hidden_st":10,"hide":6,"high":[3,5],"higher":[5,8,9,10],"how":[0,2,3,4,5,7,12],"human":4,"i":[0,1,2,3,10],"id":[3,9,10,11],"ident":3,"identifi":[2,12],"ignor":6,"implement":[3,6,7,9,10,12],"import":[2,3,5,10,12],"imposs":4,"in_featur":10,"incorpor":5,"independ":7,"indic":5,"individu":[8,12],"inf":[9,10],"infer":[0,6],"infin":6,"inform":[0,3,4,5,7,8],"inher":3,"initi":[3,5],"inject":3,"inplac":10,"input":[0,2,3,4,5,6,7,8,9,10,11,12],"input_id":10,"input_text":10,"instanc":7,"integ":[2,3,11],"internet":3,"interpret":[5,12],"intric":7,"introduc":[0,1,4,12],"item":[3,10],"iter":3,"its":[2,4,5],"izat":2,"join":7,"just":3,"k":[7,9,10],"k_":5,"kei":[6,7,10,12],"king":3,"knowledg":12,"label":3,"languag":[1,3,4,6,10,12],"larg":[1,4,7,12],"larger":9,"largest":1,"last":[3,9,10],"last_token_context_vector":[9,10],"last_token_rep":10,"later":3,"layer":[0,1,4,7,8,11],"layernorm":[4,10],"lead":[0,3,4],"learn":[0,2,3,5,6,8,12],"learnabl":[3,5,7],"learner":[1,12],"left":3,"legend":3,"len":[2,3,10],"length":[0,2,3,10,11],"less":[3,9,10],"lesson":4,"like":[3,4,6,9,10],"limit":[0,9,10],"line":10,"linear":[7,10],"linguist":[4,5],"list":[1,3],"liter":7,"llama":4,"llm":[2,4,9],"lm":[1,11,12],"lm_head":[9,10],"ln_1":10,"ln_2":10,"ln_f":10,"load":3,"loc":3,"logit":[9,10],"long":[0,4],"longer":[7,11],"look":8,"lookup":2,"loop":11,"low":[9,10],"lower":[9,10],"m":6,"machin":[0,4],"made":3,"main":1,"maintain":0,"make":[3,5],"man":3,"manag":[0,2],"mani":8,"manipul":5,"manner":1,"map":[2,3,9],"mar":2,"mask":12,"massiv":4,"match":5,"mathemat":[3,6,7],"matplotlib":3,"matric":[5,7],"matrix":[3,6,9],"maximum":11,"mean":[3,4,5,7,8,11],"meaning":3,"mechan":[0,1,4,8,12],"medium":1,"memori":0,"method":[9,12],"middl":6,"might":7,"million":1,"mlp":10,"mode":[3,10],"model":[2,3,5,6,7,8,11,12],"model_nam":10,"modern":0,"modifi":5,"modul":10,"modulelist":10,"modulenotfounderror":10,"more":[0,5,7,8,9,10,11],"most":[9,10],"most_similar":3,"mpl_toolkit":3,"mplot3d":3,"much":[5,8,9],"multi":12,"multinomi":[9,10],"multipl":7,"multipli":5,"multitask":1,"must":[6,11],"n":[3,9,10],"n_compon":3,"name":10,"natur":[3,4],"need":[1,4],"neg":6,"network":[0,3,4,12],"neural":[0,4,12],"neuron":8,"never":0,"new":[3,5,9,10,11],"newgeluactiv":10,"next":[3,4,6,7,11,12],"next_token_logit":10,"nf":10,"nmodel":10,"nn":[3,10],"no_grad":[3,10],"normal":[4,8],"normalis":4,"notic":3,"now":3,"nuanc":[7,8],"num_embed":3,"num_sampl":[9,10],"number":[0,3,10,11],"numer":[2,3,8],"nx":10,"odict_kei":10,"often":[7,8],"onc":5,"one":[1,3,4,6,9,10],"onli":[4,6,12],"onward":4,"optim":[3,4,5],"order":3,"ordereddict":10,"origin":[1,4,9,10,11],"other":5,"out":8,"out_featur":10,"output":[0,1,4,6,7,8,9,10],"output_hidden_st":10,"over":6,"overview":12,"own":4,"p":10,"pad":6,"pai":6,"paper":[0,1,4],"paragraph":4,"parallel":[0,4,7,12],"paramet":[0,1,4,8],"pars":3,"part":5,"pass":[3,4,11],"past":6,"past_key_valu":10,"pattern":[3,5,7,8],"pc1":3,"pc2":3,"pc3":3,"pca":3,"peak":6,"peek":6,"percentag":6,"perform":[4,8],"pipelin":10,"plai":5,"plot_3d_project":3,"plt":3,"poetri":4,"point":[3,4,5],"pool":[9,10],"posit":[0,5,9,10],"possibl":9,"post":4,"power":0,"pre":[3,4,9,12],"predict":[3,4,6,8,11,12],"predicted_token":10,"predicted_token_id":10,"preserv":0,"pretrain":[4,12],"previou":[4,5,6],"previous":4,"print":[2,3,9,10],"prob":10,"probabl":[5,9,10,11],"problem":4,"process":[0,2,3,4,5,6,7,8,11,12],"produc":[5,8],"program":12,"progress":4,"project":[3,5,7,9],"prompt":10,"provid":[2,5,8,12],"pt":10,"public":1,"put":5,"pyplot":3,"python":[3,12],"pytorch":12,"q":7,"qk":6,"queri":7,"rand":3,"rand_token_embed":3,"rand_token_id":3,"rand_xyz":3,"random":[3,6,9,10],"rang":[3,4],"raschka":[5,7],"rather":7,"raw":9,"re":3,"read":4,"reason":6,"receiv":[3,4,8,9],"recent":10,"recogn":7,"recurr":0,"reduc":3,"reduced_embed":3,"refer":[0,4,5,6],"refin":4,"reflect":3,"regardless":5,"regress":1,"rel":5,"relat":[3,4],"relationship":[3,5,6],"relev":5,"reli":6,"replac":6,"repres":[3,8,11],"represent":[2,3,5,8],"request":10,"requir":7,"resid_dropout":10,"residu":4,"resolut":4,"respect":6,"restrict":6,"result":[4,5],"return":[2,3,9,10],"return_tensor":10,"rich":[0,8],"richer":8,"river":3,"riverbank":3,"rnn":[0,4],"role":[3,5,8],"root":4,"rout":8,"row":3,"same":4,"sampl":[9,10],"sat":[2,3,10],"scalabl":[0,4],"scale":[0,4,6,9,10],"scaled_logit":[9,10],"scatter":[3,9,10],"scatter_":[9,10],"score":[5,6,9],"script":3,"sebastian":[5,7],"select":[9,10,11],"self":[0,1,4,8,12],"semant":[3,7],"sentenc":[2,4,5,6],"separ":7,"sequenc":[0,2,3,5,6,11,12],"sequenti":[0,4,6,11],"serv":[2,5],"set":[6,7],"set_titl":3,"set_xlabel":3,"set_ylabel":3,"set_zlabel":3,"sever":4,"shape":[3,10],"should":[5,6],"show":[3,5],"significantli":8,"similar":[3,5],"similar_word":3,"simpl":0,"simultan":[5,7,11],"singl":[7,8,11],"size":[1,2,3,4,8,9,10],"sklearn":3,"slow":0,"small":[1,3,6],"smallest":[1,3],"so":3,"softmax":[6,9,10],"softwar":3,"sole":[4,6],"sourc":[3,4,5],"space":[3,8],"special":[2,6,12],"specif":[0,3,4,5,6,9],"split":[2,7],"sqrt":6,"squar":4,"stabil":5,"stack":[4,7,11],"start":[4,5,11],"state":4,"static":5,"step":[5,6,9],"still":0,"storag":8,"store":[3,8],"strategi":9,"structur":[3,7],"sub":11,"submodul":4,"subsequ":11,"successfulli":10,"sum":[2,5,6],"summar":[2,5],"summarization_token_id":2,"surround":5,"symmetri":3,"system":2,"t":[3,4,6],"tabl":2,"take":[2,4,9],"task":4,"teach":6,"technic":6,"techniqu":0,"technologi":[0,3],"tembed":3,"temp":[9,10],"temperatur":[9,10],"tensor":[3,9,10],"term":[0,4,6],"test":2,"text":[3,4,7,10,11],"text_1":3,"text_rand":3,"than":[3,7,8],"thei":[3,5,6,11],"them":[2,5,6],"themselv":3,"therefor":0,"thi":[0,2,3,4,8,9,11,12],"those":3,"thousand":3,"three":5,"through":[3,10,11],"ti":0,"tight_layout":3,"time":[1,4,6,8],"togeth":[3,7],"token":[0,1,4,5,6,8,11,12],"token_id":[2,3],"tolist":[9,10],"top":[4,9],"top_k_indic":[9,10],"top_k_logit":[9,10],"top_n":[9,10],"top_token":10,"topk":[9,10],"topn":3,"torch":[3,9,10],"traceback":10,"track":4,"tradit":0,"train":[0,1,2,3,4,5,8,9,12],"trainabl":8,"transform":[0,2,3,5,6,7,9,10,11,12],"transformer_output":10,"translat":[0,4],"treat":3,"triangl":6,"trillion":0,"true":10,"truncat":11,"two":5,"unabl":3,"uncompress":8,"underli":4,"underlin":12,"understand":[0,1,2,3,4,6,7,8,9,12],"understood":0,"uniqu":2,"unit":[4,11],"unknown":2,"unnorm":9,"unrel":6,"unscal":5,"unstructur":3,"unsupervis":1,"up":[3,5],"updat":[3,5],"upper":[3,6],"us":[2,3,4,5,6,7,9,10],"v":[6,7],"v_":5,"valu":[3,6,7,8,9,10],"variabl":[0,2],"variou":4,"vast":0,"vector":[3,4,7,8,9,11],"veri":[9,10],"versatil":4,"via":[4,5,9],"vocab_s":3,"vocabulari":[1,2,3,9],"w_":[5,7],"wa":[0,3,4],"wai":3,"want":12,"we":[9,10],"web":3,"weight":[3,7,8],"were":[0,3,4],"what":[0,1,2,3,8],"when":[3,5],"where":[3,8],"which":6,"while":7,"why":[2,3,12],"wide":3,"within":[5,8],"without":[3,5,6],"word":[3,4,5,6,8,9,10,11,12],"word2vec":3,"word_list":3,"work":12,"would":5,"wpe":[3,10],"wpe_bank_1":3,"wpe_bank_2":3,"write":4,"wte":[3,10],"wte_bank_1":3,"wte_bank_2":3,"wv":3,"x":[3,10],"xl":1,"xy":3,"y":3,"you":[0,1,4],"z":3,"zero":8,"\ud835\udc3e_":5,"\ud835\udc44_":5,"\ud835\udc49_":5,"\ud835\udc4a_":5,"\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65":5,"\ud835\udc61\u210e\ud835\udc52":5},"titles":["Introduction to Large Language Models (LLMs)","GPT - Generative Pretrained Transformer model","Introduction to tokenization","Introduction to embedding","Transformer blocks","Self-attention mechanism","Masked Attention","Multi-head self-attention","Feedforward neural network (FNN)","Language Modeling Head (LM Head)","Pre-trained GPT-2 model end to end","Dataflow across LLM","Building Blocks of GPT-2 LLM","Reference"],"titleterms":{"2":[1,2,3,10,12],"5":10,"On":13,"The":12,"across":11,"adapt":4,"alammar":13,"anatomi":0,"appli":6,"approach":0,"architectur":4,"attent":[5,6,7],"autoregress":6,"basic":8,"block":[4,8,12],"book":13,"breakdown":3,"build":[12,13],"calcul":5,"cheat":6,"code":[5,7,9],"compon":[1,4],"context":[5,11],"credit":12,"custom":10,"d_k":5,"data":11,"dataflow":11,"decod":4,"defin":10,"demo":3,"doe":0,"dot":5,"dure":6,"each":10,"embed":3,"encod":4,"end":10,"equat":5,"experi":10,"explor":3,"extract":10,"featur":8,"feedforward":8,"field":0,"fnn":8,"frac":5,"from":13,"gener":[1,6],"gpt":[1,2,3,10,12],"grootendorst":13,"gtp":10,"hand":13,"head":[7,9,10],"help":9,"hidden":10,"how":[6,9],"i":[4,5,6,7,8,9,12],"id":2,"import":[4,6,7,8,9],"introduct":[0,2,3,12],"jai":13,"k":5,"kei":5,"languag":[0,9,13],"larg":[0,13],"layer":[3,10],"lesson":12,"limit":4,"llm":[0,3,11,12],"lm":[0,4,9,10],"load":10,"maarten":13,"main":[4,5,8],"mask":6,"matrix":5,"mechan":[5,6,7],"ml":0,"model":[0,1,4,9,10,13],"multi":7,"need":0,"network":8,"neural":8,"next":[9,10],"object":[0,1,2,3,4,5,6,7,8,9,10,11,12],"other":0,"output":[2,3],"overview":[5,8],"parallel":11,"posit":3,"pre":10,"predict":[9,10],"prerequisit":12,"pretrain":1,"prevent":6,"product":5,"q":5,"qk":5,"queri":5,"raschka":13,"refer":[12,13],"run":10,"scale":5,"scratch":13,"sebastian":13,"self":[5,6,7],"set":10,"simplest":8,"softmax":5,"solut":4,"sqrt":5,"stage":5,"state":10,"step":[2,3],"sub":2,"subword":2,"t":5,"technologi":4,"text":[2,6],"token":[2,3,9,10],"top":10,"tradit":4,"train":[6,10],"transform":[1,4,8],"tutori":12,"understand":5,"unit":2,"v":[0,5],"valu":5,"variant":1,"vector":5,"via":10,"weight":5,"what":[4,5,6,7,9],"when":6,"who":12,"why":[0,4,6,7,8,9],"wide":4,"window":11,"word":2}})