

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Self-attention mechanism &mdash; Learn LLMs  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=187304be"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=35a8b989"></script>
      <script src="../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Reference" href="../quick-reference/" />
    <link rel="prev" title="Transformer blocks" href="../05.Transformer_block/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Learn LLMs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01.LLM_intro/">Introduction to Large Language Models (LLMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02.GPT_intro/">GPT - Generative Pretrained Transformer model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03.tokenizer/">Introduction to tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04.Embeddings/">Introduction to embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05.Transformer_block/">Transformer blocks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Self-attention mechanism</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-self-attention-mechanism">What is self-attention mechanism?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#self-attention-with-q-k-v-weight-matrix">Self-attention with Q, K, V weight matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calculate-attention-weights">Calculate attention weights</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#main-stages">Main Stages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generate-context-vector">Generate context vector</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Learn LLMs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Self-attention mechanism</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/coderefinery/content/blob/main/content/06.Attention_mechanism.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="self-attention-mechanism">
<h1>Self-attention mechanism<a class="headerlink" href="#self-attention-mechanism" title="Link to this heading">ïƒ</a></h1>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<p><strong>Gain a basic understanding of:</strong></p>
<ul class="simple">
<li><p>Self-attention mechanism</p></li>
<li><p>How attention weights are calculated &amp; context vector is generated?</p></li>
</ul>
</div>
<section id="what-is-self-attention-mechanism">
<h2>What is self-attention mechanism?<a class="headerlink" href="#what-is-self-attention-mechanism" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>Self-attention: create a new, enriched representation (context vector) by incorporating information from all token embeddings in the sequence</p></li>
<li><p>Two main steps mechanism:</p>
<ol class="arabic simple">
<li><p>Scoring relevance (â€œattending toâ€/â€consideringâ€ all tokens) &amp; calculate attention weights (relevance scores)</p></li>
<li><p>Combine attention weights and generate context vector (new enriched representation)</p></li>
</ol>
</li>
<li><p>Context vector (enriched representation):</p>
<ul>
<li><p>Captures the specific meaning of a token embeddings within its surrounding embeddings</p></li>
<li><p>Allow the model to understand relationships and dependencies between words, regardless of how far apart they are in the sentence</p></li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../_images/Self-attention-mechanism.png" /></p>
</section>
<section id="self-attention-with-q-k-v-weight-matrix">
<h2>Self-attention with Q, K, V weight matrix<a class="headerlink" href="#self-attention-with-q-k-v-weight-matrix" title="Link to this heading">ïƒ</a></h2>
<p><img alt="alt text" src="../_images/Q_K_V_attention.png" />
<em>Source (modified): <a class="reference external" href="https://poloclub.github.io/transformer-explainer/">transformer-explainer</a></em></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(ğ‘„\)</span>, <span class="math notranslate nohighlight">\(ğ¾\)</span> and <span class="math notranslate nohighlight">\(V\)</span>: matrices: Representation of input token embeddings</p></li>
<li><p><span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Queries</p>
<ul>
<li><p>Token representations that are used as queries in relevance scoring (embeddings that are used as queries for the â€œcomparisonâ€)</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Keys</p>
<ul>
<li><p>Token representations that get compared to queries</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Values</p>
<ul>
<li><p>Token representations that are used to combine attention weights and generate context vector</p></li>
</ul>
</li>
</ul>
</section>
<section id="calculate-attention-weights">
<h2>Calculate attention weights<a class="headerlink" href="#calculate-attention-weights" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>Attention weights: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span></p></li>
</ul>
<section id="main-stages">
<h3>Main Stages<a class="headerlink" href="#main-stages" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p><strong>Stages 1</strong>: Dot product to calculate attention score (matrix manipulation: <span class="math notranslate nohighlight">\({QK^T}\)</span>):</p>
<ul>
<li><p>Provides unscaled attention score (initial relevance scores) - A higher dot product means the two tokens are more aligned (similar context)</p></li>
<li><p>Indicates how aligned vectors in <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> with vectors in <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span></p></li>
<li><p>i.e., how much focus <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors should put on <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors</p></li>
<li><p>Matrix manipulation enables simultaneously compare all the vectors in <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> to <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span></p></li>
</ul>
</li>
<li><p><strong>Stage 2</strong>: Scaling: Scaled attention score</p>
<ul>
<li><p>Help avoid high-values in attention score and stabilize gradients</p></li>
</ul>
</li>
<li><p><strong>Stage 3</strong>: Calculate â€œAttention weightsâ€</p>
<ul>
<li><p>Apply <code class="docutils literal notranslate"><span class="pre">softmax</span></code> function to scaled attention scores and calculate â€œAttention weightsâ€</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">softmax</span></code> function makes values to be positive and sums up 1 (convert to probabilities)</p></li>
<li><p>i.e., Convert attention scores to attention weights (probabilities) what shows â€œrelative importanceâ€ <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors put on <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors</p></li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../_images/Calculate-attention-weights.png" /></p>
</section>
</section>
<section id="generate-context-vector">
<h2>Generate context vector<a class="headerlink" href="#generate-context-vector" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>Multiply these attention weights by the Value vectors (<span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>) and produce final context vector</p></li>
</ul>
<p><img alt="alt text" src="../_images/Generate-context-vector.png" /></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(ğ‘Š_{ğ‘¡â„ğ‘’}\)</span>: To what extent token â€œtheâ€ attend to (focus on) each input token (attention weights)</p></li>
<li><p><span class="math notranslate nohighlight">\(ğ‘‰_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Representation of input token embedding matrix</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../05.Transformer_block/" class="btn btn-neutral float-left" title="Transformer blocks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../quick-reference/" class="btn btn-neutral float-right" title="Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pubudu Samarakoon.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>