Search.setIndex({"alltitles":{"About the course":[[6,"about-the-course"]],"Book \u201cBuild a Large Language Model (From Scratch)\u201d, Sebastian Raschka":[[7,"book-build-a-large-language-model-from-scratch-sebastian-raschka"]],"Building Blocks of GPT-2 LLM":[[6,null]],"Calculate attention weights":[[5,"calculate-attention-weights"]],"Credits":[[6,"credits"]],"Demo":[[3,"demo-0"],[3,"demo-1"]],"Embedding layer of LLMs":[[3,"embedding-layer-of-llms"]],"Explore LLM token embeddings":[[3,"explore-llm-token-embeddings"]],"GPT - Generative Pretrained Transformer model":[[1,null]],"GPT-2":[[1,"gpt-2"]],"GPT-2 Token embeddings: Step-by-Step Breakdown":[[3,"gpt-2-token-embeddings-step-by-step-breakdown"]],"GPT-2 tokenizer":[[2,"gpt-2-tokenizer"]],"Generate context vector":[[5,"generate-context-vector"]],"Introduction":[[6,"introduction"]],"Introduction to Large Language Models (LLMs)":[[0,null]],"Introduction to embedding":[[3,null]],"Introduction to tokenization":[[2,null]],"Key components of GPT-2 model":[[1,"key-components-of-gpt-2-model"]],"LLMs":[[0,"llms"]],"LLMs vs other ML approaches":[[0,"llms-vs-other-ml-approaches"]],"Language modeling (LM)":[[0,"language-modeling-lm"]],"Limitations and solutions":[[4,"limitations-and-solutions"]],"Limitations in traditional LM":[[4,"limitations-in-traditional-lm"]],"Main Components of a transformer block":[[4,"main-components-of-a-transformer-block"]],"Main Stages":[[5,"main-stages"]],"Objective":[[6,"objective"]],"Objectives":[[0,"objectives-0"],[1,"objectives-0"],[2,"objectives-0"],[3,"objectives-0"],[4,"objectives-0"],[5,"objectives-0"]],"Output":[[2,"instructor-note-0"],[3,"instructor-note-0"],[3,"instructor-note-1"]],"Position embeddings":[[3,"position-embeddings"]],"Prerequisites":[[6,"prerequisites-0"]],"Reference":[[6,null],[7,null]],"See also":[[6,"see-also"]],"Self-attention mechanism":[[5,null]],"Self-attention with Q, K, V weight matrix":[[5,"self-attention-with-q-k-v-weight-matrix"]],"Text to Token-IDs":[[2,"text-to-token-ids"]],"Text to sub-word units":[[2,"text-to-sub-word-units"]],"Text to subword units":[[2,"text-to-subword-units"]],"The lesson":[[6,null]],"Token embedding":[[3,"token-embedding"]],"Token embeddings in LLMs":[[3,"token-embeddings-in-llms"]],"Tokenization step-by-step":[[2,"tokenization-step-by-step"]],"Tokenizers":[[2,"tokenizers"]],"Transformer blocks":[[4,null]],"Transformer technology":[[4,"transformer-technology"]],"What is a Transformer block?":[[4,"what-is-a-transformer-block"]],"What is self-attention mechanism?":[[5,"what-is-self-attention-mechanism"]],"Who is the tutorial for?":[[6,"who-is-the-tutorial-for"]],"Why Is Transformer technology important?":[[4,"why-is-transformer-technology-important"]],"Why does the LM field needs LLMs?":[[0,"why-does-the-lm-field-needs-llms"]]},"docnames":["01.LLM_intro","02.GPT_intro","03.tokenizer","04.Embeddings","05.Transformer_block","06.Attention_mechanism","index","quick-reference"],"envversion":{"sphinx":65,"sphinx.domains.c":3,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":9,"sphinx.domains.index":1,"sphinx.domains.javascript":3,"sphinx.domains.math":2,"sphinx.domains.python":4,"sphinx.domains.rst":2,"sphinx.domains.std":2},"filenames":["01.LLM_intro.md","02.GPT_intro.md","03.tokenizer.md","04.Embeddings.md","05.Transformer_block.md","06.Attention_mechanism.md","index.md","quick-reference.md"],"indexentries":{},"objects":{},"objnames":{},"objtypes":{},"terms":{"":[0,2,3,4,6],"0":3,"0005":3,"0073":3,"0090":3,"0098":3,"0141":3,"0427":3,"0456":3,"0464":3,"0594":3,"0701":3,"0743":3,"0894":3,"0941":3,"0957":3,"0958":3,"1":[3,5],"10":3,"1052":3,"111":3,"1112":3,"1115":3,"1123":3,"12":1,"1474":3,"1542":1,"1600":1,"1634":2,"16345":2,"17":1,"17796":3,"1980":0,"1997":0,"1st":3,"2":5,"2014":0,"2015":0,"2017":1,"25192":3,"2527":3,"262":[2,3],"3":[3,5],"300":3,"318":3,"319":[2,3],"3331":3,"3332":[2,3],"3876":2,"3d":3,"4":3,"48":1,"4f":3,"5":3,"50256":3,"50257":[2,3],"5431":3,"5472":3,"58":3,"5d":3,"6":3,"62":3,"768":[1,3],"7850":3,"8":3,"84":3,"89":3,"9246":[2,3],"A":[2,5],"As":3,"At":6,"For":3,"In":[2,3],"The":[1,2,3,4],"These":[0,2,3],"To":5,"With":0,"_":3,"abil":0,"accept":2,"access":3,"across":4,"ad":[2,3],"add_subplot":3,"address":[0,4],"advanc":0,"align":5,"all":[0,1,3,4,5],"allow":[4,5],"alon":3,"ambigu":4,"amount":0,"anim":3,"anoth":4,"apart":5,"api":3,"appear":3,"appli":[3,5],"appropri":4,"ar":[0,1,2,3,5],"arbitrari":3,"architectur":[0,1,4,6],"aren":3,"art":4,"assign":[2,3],"attend":5,"attent":[0,1,3,4,6],"attribut":3,"auto":1,"automodelforcausallm":3,"autotoken":2,"avoid":5,"ax":3,"axes3d":3,"back":3,"backpropag":3,"balanc":2,"bank":3,"bank_1_id":3,"bank_2_id":3,"base":[0,1,4],"basic":[4,5,6],"befor":3,"behavior":0,"behaviour":4,"being":0,"bert":4,"between":[3,5],"billion":0,"bite":3,"block":[1,3],"both":4,"boundari":2,"break":3,"build":[0,1,2],"calcul":3,"can":[0,1,2,3,4],"capabl":0,"captur":[3,5],"case":2,"cat":[2,3],"challeng":0,"cluster":3,"code":4,"coher":4,"combin":[3,5],"compar":5,"comparison":5,"complex":[0,3],"compon":6,"comprehens":6,"comput":[0,2,4],"concept":6,"consid":5,"consist":2,"construct":4,"context":3,"contextu":[3,4],"continu":3,"convert":[2,3,5],"core":0,"corpu":1,"correspond":2,"cosine_sim":3,"cosine_similar":3,"couldn":3,"creat":5,"d_k":5,"data":[0,1,2,3,4,6],"dataflow":6,"decod":[0,2,3],"decoded_sent":2,"decomposit":3,"deep":[2,6],"deeper":6,"def":3,"dens":3,"depend":[0,4,5],"design":6,"despit":0,"detail":3,"determin":4,"dictionari":2,"differ":[0,3],"differenti":[3,4],"dim":3,"dimens":[1,3],"dimension":3,"discret":[2,3],"distinguish":3,"dive":6,"divers":2,"do":[0,3],"document":2,"doe":3,"dog":3,"dot":5,"download":3,"dure":3,"e":[0,2,3,5,6],"each":[2,3,4,5,6],"earli":0,"effect":[2,3,4],"effici":2,"element":3,"elimin":0,"embed":[1,5,6],"embedding_dim":3,"emerg":0,"emploi":0,"enabl":[2,3,4,5,6],"encod":[0,2,3],"end":[4,6],"enrich":5,"entir":3,"enumer":3,"equal":[2,3],"error":3,"evalu":3,"evolv":3,"exampl":3,"exhibit":0,"explain":[3,5],"explicitli":0,"explor":6,"express":2,"extend":3,"extent":5,"extract":3,"f":[2,3],"fail":4,"far":5,"feed":4,"feedforward":1,"fig":3,"figsiz":3,"figur":3,"file":0,"final":[3,5],"first":3,"fit_transform":3,"fix":2,"flow":3,"focu":[5,6],"follow":3,"fontsiz":3,"forward":4,"foundat":2,"four":1,"frac":5,"from":[0,2,3,4,5,6],"from_pretrain":[2,3],"fulli":0,"function":[3,5],"fundament":[4,6],"g":[0,3],"gain":[4,5,6],"gate":0,"gener":[2,3,4,6],"gensim":3,"geometr":3,"get":[3,5],"get_3d_project":3,"given":3,"googl":3,"gpt":4,"gpt2":[2,3],"gradient":[0,3,5],"ha":3,"handl":[0,2],"happen":3,"have":[3,6],"head":1,"help":[3,5,6],"high":[3,5],"higher":5,"how":[0,2,3,4,5,6],"human":4,"i":[0,1,2,3],"id":3,"ident":3,"identifi":2,"implement":[3,6],"import":[2,3,5,6],"imposs":4,"incorpor":5,"indic":5,"individu":6,"infer":0,"inform":[0,3,5],"inher":3,"initi":[3,5],"inject":3,"input":[0,2,3,4,5,6],"integ":[2,3],"internet":3,"interpret":6,"introduc":[0,1,4,6],"item":3,"iter":3,"its":[4,5],"izat":2,"just":3,"kei":[5,6],"king":3,"knowledg":6,"label":3,"languag":[1,3,4,6],"larg":[1,4,6],"largest":1,"last":3,"later":3,"layer":[1,4],"lead":[0,3,4],"learn":[0,2,3,6],"learnabl":3,"learner":[1,6],"left":3,"legend":3,"len":[2,3],"length":[0,2,3],"less":3,"like":[3,4],"limit":0,"linguist":4,"link":6,"list":[1,3],"llama":4,"llm":[2,4],"load":3,"loc":3,"long":[0,4],"lookup":2,"machin":0,"made":3,"main":1,"maintain":0,"make":[3,5],"man":3,"manag":[0,2],"manipul":5,"manner":1,"map":[2,3],"mar":2,"massiv":4,"mathemat":3,"matplotlib":3,"matric":5,"matrix":3,"mean":[3,5],"meaning":3,"mechan":[0,1,4,6],"memori":0,"method":6,"million":1,"mode":3,"model":[2,3,4,5,6],"modern":0,"modifi":5,"more":[0,5],"most_similar":3,"mpl_toolkit":3,"mplot3d":3,"much":5,"multipli":5,"multitask":1,"n":3,"n_compon":3,"natur":3,"need":[1,4],"network":[0,1,3,4],"neural":[0,1,4],"never":0,"new":[3,5],"next":[3,4,6],"nn":3,"no_grad":3,"notic":3,"now":3,"num_embed":3,"number":[0,3],"numer":[2,3],"one":[1,3,4],"optim":3,"order":3,"origin":1,"output":0,"overal":6,"overview":6,"paper":[0,1,4],"paragraph":4,"parallel":[0,4,6],"paramet":[0,1,4],"pars":3,"pass":[3,4],"pattern":3,"pc1":3,"pc2":3,"pc3":3,"pca":3,"perform":4,"plot_3d_project":3,"plt":3,"point":3,"posit":[0,5],"power":0,"pre":3,"predict":[3,6],"preserv":0,"pretrain":6,"previou":4,"previous":4,"print":[2,3],"probabl":5,"problem":4,"process":[0,2,3,4,6],"produc":5,"product":5,"program":6,"progress":4,"project":3,"provid":[2,5,6],"public":1,"put":5,"pyplot":3,"python":[3,6],"pytorch":6,"qk":5,"queri":5,"rand":3,"rand_token_embed":3,"rand_token_id":3,"rand_xyz":3,"random":3,"rang":[3,4],"re":3,"reader":6,"receiv":[3,4],"recurr":0,"reduc":3,"reduced_embed":3,"refer":[0,4],"refin":4,"reflect":3,"regardless":5,"regress":1,"rel":5,"relat":[3,4],"relationship":[3,5],"relev":5,"repres":3,"represent":[2,3,5],"resolut":4,"resourc":6,"result":4,"return":[2,3],"rich":0,"river":3,"riverbank":3,"rnn":[0,4],"role":3,"row":3,"same":4,"sat":[2,3],"scalabl":[0,4],"scale":[0,4,5],"scatter":3,"score":5,"script":3,"self":[0,1,4,6],"semant":3,"sentenc":[2,4,5],"sequenc":[0,2,3,5,6],"sequenti":0,"serv":2,"set_titl":3,"set_xlabel":3,"set_ylabel":3,"set_zlabel":3,"sever":4,"shape":3,"should":5,"show":[3,5],"similar":[3,5],"similar_word":3,"simpl":0,"simultan":5,"size":[2,3,4],"sklearn":3,"slow":0,"small":3,"smallest":[1,3],"so":3,"softmax":5,"softwar":3,"sourc":[3,5],"space":3,"special":[2,6],"specif":[0,3,4,5],"split":2,"sqrt":5,"stabil":5,"stack":4,"start":4,"state":4,"step":5,"still":0,"store":3,"structur":3,"sum":[2,5],"summar":2,"summarization_token_id":2,"surround":5,"symmetri":3,"system":2,"t":[3,4,5],"tabl":2,"take":2,"task":4,"techniqu":0,"technologi":[0,3],"tembed":3,"tensor":3,"term":[0,4],"test":2,"text":[3,4],"text_1":3,"text_rand":3,"than":3,"thei":[3,5],"them":2,"themselv":3,"therefor":0,"thi":[0,2,3,6],"those":3,"thousand":3,"through":3,"ti":0,"tight_layout":3,"time":1,"togeth":3,"token":[0,1,4,5,6],"token_id":[2,3],"top":4,"topn":3,"torch":3,"track":4,"tradit":0,"train":[0,1,2,3,4],"transform":[0,2,3,5,6],"translat":[0,4],"treat":3,"trillion":0,"two":5,"unabl":3,"underli":4,"underlin":6,"understand":[0,1,2,3,4,5,6],"understood":0,"uniqu":2,"unit":4,"unknown":2,"unscal":5,"unstructur":3,"unsupervis":1,"up":[3,5],"updat":3,"upper":3,"us":[2,3,4,5],"v_":5,"valu":[3,5],"variabl":[0,2],"variou":4,"vast":0,"vector":3,"versatil":4,"via":4,"vocab_s":3,"vocabulari":[2,3],"wa":[0,3,4],"wai":3,"want":6,"web":3,"weight":3,"were":[0,3,4],"what":[0,1,2,3],"when":3,"where":3,"why":[2,3,6],"wide":[3,4],"within":5,"without":3,"word":[3,4,5,6],"word2vec":3,"word_list":3,"work":6,"wpe":3,"wpe_bank_1":3,"wpe_bank_2":3,"wte":3,"wte_bank_1":3,"wte_bank_2":3,"wv":3,"x":3,"xy":3,"y":3,"you":[0,1,4],"z":3,"\ud835\udc3e":5,"\ud835\udc3e_":5,"\ud835\udc44":5,"\ud835\udc44_":5,"\ud835\udc49_":5,"\ud835\udc4a_":5,"\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65":5,"\ud835\udc61\u210e\ud835\udc52":5},"titles":["Introduction to Large Language Models (LLMs)","GPT - Generative Pretrained Transformer model","Introduction to tokenization","Introduction to embedding","Transformer blocks","Self-attention mechanism","Building Blocks of GPT-2 LLM","Reference"],"titleterms":{"2":[1,2,3,6],"The":6,"about":6,"also":6,"approach":0,"attent":5,"block":[4,6],"book":7,"breakdown":3,"build":[6,7],"calcul":5,"compon":[1,4],"context":5,"cours":6,"credit":6,"demo":3,"doe":0,"embed":3,"explor":3,"field":0,"from":7,"gener":[1,5],"gpt":[1,2,3,6],"i":[4,5,6],"id":2,"import":4,"introduct":[0,2,3,6],"k":5,"kei":1,"languag":[0,7],"larg":[0,7],"layer":3,"lesson":6,"limit":4,"llm":[0,3,6],"lm":[0,4],"main":[4,5],"matrix":5,"mechan":5,"ml":0,"model":[0,1,7],"need":0,"object":[0,1,2,3,4,5,6],"other":0,"output":[2,3],"posit":3,"prerequisit":6,"pretrain":1,"q":5,"raschka":7,"refer":[6,7],"scratch":7,"sebastian":7,"see":6,"self":5,"solut":4,"stage":5,"step":[2,3],"sub":2,"subword":2,"technologi":4,"text":2,"token":[2,3],"tradit":4,"transform":[1,4],"tutori":6,"unit":2,"v":[0,5],"vector":5,"weight":5,"what":[4,5],"who":6,"why":[0,4],"word":2}})