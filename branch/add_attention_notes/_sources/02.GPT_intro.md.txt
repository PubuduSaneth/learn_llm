# GPT - Generative Pretrained Transformer model

:::{objectives}

**Understand:**

- What is GPT?
- What are the main components (building blocks) of GPT-2 model?

:::

- **Generative**: The model can generate tokens auto-regressive manner (generate one token at a time)
- **Pretrained**: Trained on a large corpus of data
- **Transformer**: The model architecture is based on the transformer, introduced in the 2017 paper [“Attention is All You Need” (Self-Attention Mechanism)](https://arxiv.org/abs/1706.03762)

## GPT-2

- Original publication: ["Language Models are Unsupervised Multitask Learners"](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- GPT-2 original publication lists four models
  - Smallest GPT-2 model:
    - 17 million parameters; 12 transformer blocks; Model dimensions: 768
  - Largest GPT-2 model:
    - 1542 million parameters; 48 transformer blocks; Model dimensions: 1600

![alt text](images/gpt2.png)

## Key components of GPT-2 model

- Tokenizer
- Embedding layer
- Transformer block
  - Self-attention layer
  - Feedforward neural network
- Language modeling head
