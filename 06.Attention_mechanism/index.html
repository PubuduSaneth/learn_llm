

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Self-attention mechanism &mdash; Learn LLMs  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=187304be"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=35a8b989"></script>
      <script src="../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=1ae7504c"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Masked Attention" href="../07.masked_attention/" />
    <link rel="prev" title="Transformer blocks" href="../05.Transformer_block/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Learn LLMs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01.LLM_intro/">Introduction to Large Language Models (LLMs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02.GPT_intro/">GPT - Generative Pretrained Transformer model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03.tokenizer/">Introduction to tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04.Embeddings/">Introduction to embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05.Transformer_block/">Transformer blocks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Self-attention mechanism</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-self-attention-mechanism">What is self-attention mechanism?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#understanding-self-attention-with-q-k-v-weight-matrix">Understanding self-attention with Q, K, V weight matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview-of-self-attention-with-q-k-v-weight-matrix">Overview of self-attention with Q, K, V weight matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#queries-q-keys-k-and-values-v">Queries (Q), Keys (K), and Values (V)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#context-vector">Context vector</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#understanding-attention-equation-scaled-dot-product-attention">Understanding attention equation (scaled Dot-Product Attention)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#attention-weights-calculation-softmax-frac-qk-t-sqrt-d-k">Attention weights calculation: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#main-stages-softmax-frac-qk-t-sqrt-d-k-v">Main Stages: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})*V\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../07.masked_attention/">Masked Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08.Multihead-attention/">Multi-head self-attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09.FNN/">Feedforward neural network (FNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10.LM_head/">Language Modeling Head (LM Head)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_1.LLM-end-to-end/">Pre-trained GPT-2 model end to end</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11.LLM_dataflow/">Dataflow across LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Learn LLMs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Self-attention mechanism</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/pubudusaneth/learn_llm/blob/main/content/06.Attention_mechanism.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="self-attention-mechanism">
<h1>Self-attention mechanism<a class="headerlink" href="#self-attention-mechanism" title="Link to this heading">ïƒ</a></h1>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Gain a basic understanding of self-attention mechanism</p></li>
<li><p>Explore how attention weights are calculated &amp; how context vector is generated.</p></li>
</ul>
</div>
<section id="what-is-self-attention-mechanism">
<h2>What is self-attention mechanism?<a class="headerlink" href="#what-is-self-attention-mechanism" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>Self-attention: create a new, enriched representation (context vector) by incorporating information from all token embeddings in the sequence</p></li>
<li><p>Two main steps mechanism:</p>
<ol class="arabic simple">
<li><p>Scoring relevance (â€œattending toâ€/â€consideringâ€ all tokens) &amp; calculate attention weights (relevance scores)</p></li>
<li><p>Combine attention weights and generate context vector (new enriched representation)</p></li>
</ol>
</li>
<li><p>Context vector (context-aware representation / enriched representation):</p>
<ul>
<li><p>Captures the specific meaning of a token embeddings within its surrounding embeddings</p></li>
<li><p>Allow the model to understand relationships and dependencies between words, regardless of how far apart they are in the sentence</p></li>
</ul>
</li>
</ul>
<p><img alt="alt text" src="../_images/Self-attention-mechanism.png" /></p>
</section>
<section id="understanding-self-attention-with-q-k-v-weight-matrix">
<h2>Understanding self-attention with Q, K, V weight matrix<a class="headerlink" href="#understanding-self-attention-with-q-k-v-weight-matrix" title="Link to this heading">ïƒ</a></h2>
<section id="overview-of-self-attention-with-q-k-v-weight-matrix">
<h3>Overview of self-attention with Q, K, V weight matrix<a class="headerlink" href="#overview-of-self-attention-with-q-k-v-weight-matrix" title="Link to this heading">ïƒ</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Key concepts of attention mechanism</p>
<ul>
<li><p><em>Q (Query), K (Key) and V (Value)</em> roles input token embeddings play in attention mechanism</p></li>
<li><p><em>Q, K, V projection</em> via <em>learnable projection matrices</em> (<span class="math notranslate nohighlight">\(W_{k}\)</span>, <span class="math notranslate nohighlight">\(W_{q}\)</span>, <span class="math notranslate nohighlight">\(W_{v}\)</span>)</p></li>
<li><p><em>Attention scores</em> and <em>Attention weights</em></p></li>
<li><p><em>Context vectors</em></p></li>
</ul>
</li>
</ul>
</div>
<p><img alt="alt text" src="../_images/Q_K_V_attention_explainer.png" />
<em>Source (modified): <a class="reference external" href="https://poloclub.github.io/transformer-explainer/">transformer-explainer</a></em></p>
</section>
<section id="queries-q-keys-k-and-values-v">
<h3>Queries (Q), Keys (K), and Values (V)<a class="headerlink" href="#queries-q-keys-k-and-values-v" title="Link to this heading">ïƒ</a></h3>
<p><strong>Three roles of input token embeddings: Queries (Q), Keys (K), and Values (V):</strong></p>
<ul class="simple">
<li><p>Input token embeddings play three distinct roles in the attention mechanism</p></li>
<li><p><em>Queries (Q)</em> â€œattendâ€ to other vectors in the input sequence and serve as starting points to generate context vectors</p></li>
<li><p><em>Keys (K)</em> gets compared to (matched with) <em>Queries (Q)</em> when calculating attention scores (relevance scores)</p></li>
<li><p><em>Values (V)</em> serve as â€œreference valuesâ€ that summarize scores from all-vs-all <em>Q</em> and <em>K</em> matching</p></li>
</ul>
<p><strong>Q, K, V projection:</strong></p>
<ul class="simple">
<li><p>First step in attention mechanism is to project <em>Q, K, V embeddings</em> to <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>, <span class="math notranslate nohighlight">\(K_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> and <span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> representations via a <em>learnable projection matrices</em></p></li>
<li><p><span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Vectors that â€œattendâ€ to other vectors in the input sequence. (Actual vectors that are used as queries for the â€œcomparison / attentionâ€)</p></li>
<li><p><span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Vectors that get compared to queries</p></li>
<li><p><span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Vectors that are used to combine values from previous step and generate context vector</p></li>
<li><p><span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>, <span class="math notranslate nohighlight">\(K_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> and <span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> have similar dimensions to Q, K, V embeddings, but the values in these matrices can get updated during the model training (due to the <em>learnable projection matrices:</em> <span class="math notranslate nohighlight">\(W_{q}\)</span>, <span class="math notranslate nohighlight">\(W_{k}\)</span> and <span class="math notranslate nohighlight">\(W_{v}\)</span>)</p></li>
</ul>
<p><strong>Learnable projection matrices:</strong> <span class="math notranslate nohighlight">\(W_{q}\)</span>, <span class="math notranslate nohighlight">\(W_{k}\)</span> and <span class="math notranslate nohighlight">\(W_{v}\)</span></p>
<ul class="simple">
<li><p>Without these projection matrices, the relationship between embeddings would be static</p></li>
<li><p>The learnable matrices ensure that the attention mechanism is dynamic and allows <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>, <span class="math notranslate nohighlight">\(K_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> and <span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> get updated in model training</p></li>
<li><p>Allow the model to learn how to interpret embeddings differently depending on their context in the input sequence</p>
<ul>
<li><p>Learnability allow the model to discover and optimize complex linguistic patterns during training</p></li>
</ul>
</li>
</ul>
<p><strong>Attention weights:</strong></p>
<ul class="simple">
<li><p>Attention weights are calculated by:</p>
<ol class="arabic simple">
<li><p>Multiplies the Query vectors by the all the Key vectors in the sequence</p></li>
<li><p>Scale these scores</p></li>
<li><p>Convert scaled scores to probabilities</p></li>
</ol>
</li>
<li><p>These probabilities (attention weights) help model determine how much focus the current token should put on other tokens</p></li>
</ul>
</section>
<section id="context-vector">
<h3>Context vector<a class="headerlink" href="#context-vector" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>Once attention weights are calculated, the model uses them to aggregate information.</p></li>
<li><p>The result is a new, enriched vectors that contain context from the relevant parts of the sequence</p></li>
</ul>
</section>
</section>
<section id="understanding-attention-equation-scaled-dot-product-attention">
<h2>Understanding attention equation (scaled Dot-Product Attention)<a class="headerlink" href="#understanding-attention-equation-scaled-dot-product-attention" title="Link to this heading">ïƒ</a></h2>
<ul class="simple">
<li><p>Attention scores: (<span class="math notranslate nohighlight">\({QK^T}\)</span>)</p></li>
<li><p>Attention weights: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span></p></li>
<li><p>Context vector calculation: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})*V\)</span></p></li>
</ul>
<section id="attention-weights-calculation-softmax-frac-qk-t-sqrt-d-k">
<h3>Attention weights calculation: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span><a class="headerlink" href="#attention-weights-calculation-softmax-frac-qk-t-sqrt-d-k" title="Link to this heading">ïƒ</a></h3>
<p><strong>Three stage Attention weights calculation process:</strong></p>
<ul class="simple">
<li><p>Stages 1:  Calculate attention score with <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code> (<span class="math notranslate nohighlight">\({QK^T}\)</span>)</p></li>
<li><p>Stage 2: Scaling (<span class="math notranslate nohighlight">\(\frac{QK^T}{\sqrt{d_k}}\)</span>)</p></li>
<li><p>Stage 3: Calculate â€œAttention weightsâ€ (<span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span>)</p></li>
</ul>
<p><img alt="alt text" src="../_images/Calculate-attention-weights.png" /></p>
<p><strong>Stages 1:  Calculate attention score with <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code> (<span class="math notranslate nohighlight">\({QK^T}\)</span>):</strong></p>
<ul class="simple">
<li><p>Provides unscaled attention score (initial relevance scores) - A higher dot product means the two tokens are more aligned (similar context)</p></li>
<li><p>Indicates how aligned vectors in <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> with vectors in <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span></p></li>
<li><p>i.e., how much focus <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors should put on <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors</p></li>
<li><p>Matrix manipulation enables simultaneously compare all the vectors in <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> to <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span></p></li>
</ul>
<p><strong>Stage 2: Scaling (<span class="math notranslate nohighlight">\(\frac{QK^T}{\sqrt{d_k}}\)</span>):</strong></p>
<ul class="simple">
<li><p>Scaled attention scores</p></li>
<li><p>Help avoid high-values in attention score and stabilize gradients</p></li>
</ul>
<p><strong>Stage 3: Calculate â€œAttention weightsâ€ (<span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span>):</strong></p>
<ul class="simple">
<li><p>Apply <code class="docutils literal notranslate"><span class="pre">softmax</span></code> function to scaled attention scores and calculate â€œAttention weightsâ€</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">softmax</span></code> function makes values to be positive and sums up 1 (convert to probabilities)</p></li>
<li><p>i.e., Convert attention scores to attention weights (probabilities) what shows â€œrelative importanceâ€ <span class="math notranslate nohighlight">\(ğ‘„_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors put on <span class="math notranslate nohighlight">\(ğ¾_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span> vectors</p></li>
</ul>
</section>
<section id="main-stages-softmax-frac-qk-t-sqrt-d-k-v">
<h3>Main Stages: <span class="math notranslate nohighlight">\(softmax(\frac{QK^T}{\sqrt{d_k}})*V\)</span><a class="headerlink" href="#main-stages-softmax-frac-qk-t-sqrt-d-k-v" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p>Multiply these attention weights by the Value vectors (<span class="math notranslate nohighlight">\(V_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>) and produce final context vector</p></li>
</ul>
<p><img alt="alt text" src="../_images/Generate-context-vector.png" /></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(ğ‘Š_{ğ‘¡â„ğ‘’}\)</span>: To what extent token â€œtheâ€ attend to (focus on) each input token (attention weights)</p></li>
<li><p><span class="math notranslate nohighlight">\(ğ‘‰_{ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥}\)</span>: Representation of input token embedding matrix</p></li>
</ul>
<div class="admonition-coding instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Coding</p>
<p><strong>Coding Attention Mechanisms:</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb">Chapter 3: Coding Attention Mechanisms by by Sebastian Raschka</a></p></li>
</ul>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../05.Transformer_block/" class="btn btn-neutral float-left" title="Transformer blocks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../07.masked_attention/" class="btn btn-neutral float-right" title="Masked Attention" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Pubudu Samarakoon, 2026.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>