# Masked Attention

:::{objectives}

- Understand what Masked Self-Attention
- How to apply Self-Attention mask

:::

## What is Masked Self-Attention?

- Mechanism specific to the **decoder** blocks of a Transformer (like GPT).
- Restricts the model's to **"only pay attention to previous token embeddings"** when it is processing a specific token in the sequence
- Masks model's ability to **pay to any future token embeddings**

## Why is it Important?

|                         | Without *mask*           | With *mask*                  |
| ----------------------- | ------------------------ | ---------------------------- |
|**Attention mechanism**  | Exposed to future tokens | Not exposed to future tokens |
|**Training & Inference** | Attend to future tokens  | Predict future tokens        |

### Prevents "Cheating" During Training

- During training, model should learn how to predict the next token without “peeking” into the future
- Masked attention ensures the model relies solely on the past to predict the future

### Autoregressive text generation

- Key to enable sequential generation autoregressive text generation** (predicting the next word)
- Forces the model to generate text one token at a time, feeding the output of one step as the input for the next, which is essential for creating coherent sentences.

## Masking during the attention mechanism

### When is the mask applied?

- Mask is applied to scaled attention scores
- Masked attention scores are used in the Softmax function

![alt text](images/masked_attention.png)

### How to apply attention mask?

- Mask: Set upper triangle values of the attention matrix to negative infinity
- $softmax(\frac{QK^T}{\sqrt{d_k}} + M)*V$

![alt text](images/masking.png)