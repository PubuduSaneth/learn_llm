# Language Modeling Head (LM Head)

:::{objectives}

- Understand what a LM head is
- Explore how LM head help predict the next token

:::

## What is LM head?

- LM head projects the Context vector of the last token from the final block into the size of the model's vocabulary and calculates a **probability score** for every possible next token
- Enable next token prediction

## Why LM head is important?

- LM head is final component of a Transformer model, specifically in a LLM like GPT
- Connects the output of the final Transformer block to the model's vocabulary & help predict the next token

## How is the LM head help predict next token

![alt text](images/LM_head.png)

- Input: Receives the "output vector" from the last Transformer block
  - E.g., if the model uses an embedding size of 3,072, the LM head takes in a vector of size 3,072 for each token.
- Maps this input vector to a much larger vector equal to the size of the model's vocabulary (e.g., 50,257 for GPT-2)
  - These outputs are raw, unnormalized scores called `logits`
- Convert `logits` to probability scores via softmax function
- Employ a sampling strategy to select the next token prediction
