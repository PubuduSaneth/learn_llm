# Transformer blocks

:::{objectives}

**Gain a basic understanding of:**

- Transformer technology and why it is important?
- Transformer block and its main components?

:::

## Limitations in traditional LM

- RNN based traditional LM failed to track long-range dependencies like understanding how a word at the start of a paragraph relates to one at the end
- RNN based models that processed words one by one (not scalable)
- **Ambiguity Resolution**: Can't differentiate specific linguistic problems like determining what "it" refers to in several sentences

## Transformer technology

- Transformer technology was introduced in the paper “Attention Is All You Need” to address several limitations in [RNN](01.LLM_intro.md) based language modeling (LM)

### Limitations and solutions

| Limitation                   | Solution                                                |
| ---------------------------- | ------------------------------------------------------- |
| Long-range dependencies      | Contextual Understanding via self-attention mechanism   |
| not scalable                 | Parallel Processing of tokens                           |
| specific linguistic problems | Ambiguity Resolution via self-attention mechanism       |

## Why Is Transformer technology important?

- Scalability:
  - Allows for massive scaling (in terms of parameters and training data size)
  - Leading to the "Large" in LLMs.
- Architectural versatility:
  - The same underlying transformer block architecture is used across various state-of-the-art models (like GPT, Llama, and BERT)
- Versatility performance/behaviour:
  - Enables models to generate coherent, contextually appropriate text and perform a wide range of tasks—from translation to coding—that were previously impossible for computers
  - Effective for both understanding and generating human language

## Transfomer model - Encoder and Decoder

- <Text>

## What is a Transformer block?

- Transformer block is the fundamental architectural unit of a LLMs
- LLMs - constructed by stacking these blocks on top of one another
  - Each block processes the input it receives from the previous layer and passes the result to the next
  - Stacked transformer blocks progressively refining the model's understanding of the text

![alt text](images/stacked-transformer-blocks.png)

## Main Components of a transformer block

- Attention mechanism
- Feed Forward neural Network
